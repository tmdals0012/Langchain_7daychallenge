{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introdution\n",
    "Python 의 Model I/O에 대해 학습\n",
    "\n",
    "# FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model = \"gpt-4.1-nano\",\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks = [\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know this:\n",
      "Capital: Madrid  \n",
      "Language: Spanish  \n",
      "Food: Paella and Tapas  \n",
      "Currency: Euro"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I know this:\\nCapital: Madrid  \\nLanguage: Spanish  \\nFood: Paella and Tapas  \\nCurrency: Euro', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run--7c0c04af-6df5-4f95-98dc-e4ab57b394b5-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\" : \"Spain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Spain is known for its rich cultural heritage, including flamenco music and dance, delicious cuisine such as paella and tapas, and historical landmarks like the Alhambra and Sagrada Familia. It is also famous for its vibrant festivals, such as La Tomatina and Running of the Bulls. Additionally, Spain has diverse regions, each with its own unique traditions and languages, such as Catalonia and the Basque Country."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: Spain is known for its rich cultural heritage, including flamenco music and dance, delicious cuisine such as paella and tapas, and historical landmarks like the Alhambra and Sagrada Familia. It is also famous for its vibrant festivals, such as La Tomatina and Running of the Bulls. Additionally, Spain has diverse regions, each with its own unique traditions and languages, such as Catalonia and the Basque Country.', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run--09894610-cbe2-49ac-9695-72fd6bd25224-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 1. Define few-shot examples\n",
    "examples = [\n",
    "    {\"question\": \"What do you know about France?\", \"answer\": \"It is known for the Eiffel Tower.\"},\n",
    "    {\"question\": \"What do you know about Japan?\", \"answer\": \"It is famous for Mount Fuji and sushi.\"}\n",
    "]\n",
    "\n",
    "# 2. Define example format\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# 3. Define few-shot prompt template\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "# 4. Define chat model\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "# 5. Combine prompt and chat in a chain\n",
    "chain = prompt | chat\n",
    "\n",
    "# 6. Run the chain\n",
    "chain.invoke({\"country\": \"Spain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",         # Specifies the OpenAI model\n",
    "    temperature=0.1,             # Low randomness; more deterministic responses\n",
    "    streaming=True,              # Enables streaming responses token-by-token\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()  # Streams output to standard output\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"what do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"what do you know abiut {country}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea refers to two countries: \n",
      "\n",
      "1. **South Korea** (Republic of Korea)\n",
      "   - Capital: Seoul\n",
      "   - Language: Korean\n",
      "   - Currency: South Korean Won (KRW)\n",
      "\n",
      "2. **North Korea** (Democratic People's Republic of Korea)\n",
      "   - Capital: Pyongyang\n",
      "   - Language: Korean\n",
      "   - Currency: North Korean Won (KPW)\n",
      "\n",
      "Both countries share a common cultural and historical background but have distinct political systems."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Korea refers to two countries: \\n\\n1. **South Korea** (Republic of Korea)\\n   - Capital: Seoul\\n   - Language: Korean\\n   - Currency: South Korean Won (KRW)\\n\\n2. **North Korea** (Democratic People's Republic of Korea)\\n   - Capital: Pyongyang\\n   - Language: Korean\\n   - Currency: North Korean Won (KPW)\\n\\nBoth countries share a common cultural and historical background but have distinct political systems.\", additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run--e0a7b394-6e2f-4a44-bf75-44264dbd3db2-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain: any= final_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"country\", \"korea\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengthBasedExampleSelector\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt: PromptTemplate = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about France?\\nAI: \\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Here is what I know about Spain:\n",
      "\n",
      "- **Capital**: Madrid\n",
      "- **Language**: Spanish (Castilian)\n",
      "- **Food**: Tapas, Paella, Jamón Ibérico\n",
      "- **Currency**: Euro\n",
      "- **Culture**: Known for its rich history, art, flamenco music and dance, and festivals like La Tomatina and Running of the Bulls.\n",
      "- **Geography**: Located in Southwestern Europe, bordered by France, Portugal, and the Mediterranean Sea. \n",
      "\n",
      "If you have specific aspects of Spain you'd like to know more about, feel free to ask!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"AI: Here is what I know about Spain:\\n\\n- **Capital**: Madrid\\n- **Language**: Spanish (Castilian)\\n- **Food**: Tapas, Paella, Jamón Ibérico\\n- **Currency**: Euro\\n- **Culture**: Known for its rich history, art, flamenco music and dance, and festivals like La Tomatina and Running of the Bulls.\\n- **Geography**: Located in Southwestern Europe, bordered by France, Portugal, and the Mediterranean Sea. \\n\\nIf you have specific aspects of Spain you'd like to know more about, feel free to ask!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'service_tier': 'default'}, id='run--5abbb55a-6a8e-4691-82e5-b75c2f2d6882-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Spain\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomBasedExampleSelector\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model = \"gpt-4.1-nano\",\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks = [\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples: Any = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialization_and_Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1720\\1506992078.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model = \"gpt-4.1-nano\",\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks = [\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1720\\1870182591.py:44: LangChainDeprecationWarning: This class is deprecated in favor of chaining individual prompts together.\n",
      "  full_prompt = PipelinePromptTemplate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr! That be a fine question, matey! I be lovin' a hearty feast of salted fish and grog, aye! Nothing beats a good ol' pirate's meal after a day of plunderin' the high seas! Yarrr!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Arrr! That be a fine question, matey! I be lovin' a hearty feast of salted fish and grog, aye! Nothing beats a good ol' pirate's meal after a day of plunderin' the high seas! Yarrr!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run--41c8cccc-a155-41c2-aae7-8e5aec2d8998-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonation a {character}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    this is an example of how you talk:\n",
    "\n",
    "    Huamn: {example_question}\n",
    "    You: {example_answer}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "final : PromptTemplate = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts\n",
    ")\n",
    "\n",
    "chain: Any = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germmany'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks = [\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "\n",
    "prompt.format(country=\"Germmany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'service_tier': 'default'}, id='run--f1bd0873-11a1-4356-8903-631f6f557f85-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"country\" : \"Germmany\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: what is the captal of Japan\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [660ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The capital of Japan is Tokyo.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The capital of Japan is Tokyo.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 7,\n",
      "                \"prompt_tokens\": 14,\n",
      "                \"total_tokens\": 21,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_560af6e559\",\n",
      "              \"id\": \"chatcmpl-CKgINeljfT14IA7PzpaMsPumC87Pj\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--73b62f32-834f-4c79-9c09-8b008650c9ac-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 14,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 21,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 7,\n",
      "      \"prompt_tokens\": 14,\n",
      "      \"total_tokens\": 21,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_560af6e559\",\n",
      "    \"id\": \"chatcmpl-CKgINeljfT14IA7PzpaMsPumC87Pj\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Japan is Tokyo.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caching\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "chat.predict(\"what is the captal of Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: what is the captal of Korea\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1.38s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The capital of South Korea is Seoul. North Korea's capital is Pyongyang.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The capital of South Korea is Seoul. North Korea's capital is Pyongyang.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 17,\n",
      "                \"prompt_tokens\": 14,\n",
      "                \"total_tokens\": 31,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_560af6e559\",\n",
      "              \"id\": \"chatcmpl-CKgIR2ma9W6wi3YJlvetwG9wGIYsi\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--a62e6885-f845-4ed1-b9bf-06ed847e2dab-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 14,\n",
      "              \"output_tokens\": 17,\n",
      "              \"total_tokens\": 31,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 17,\n",
      "      \"prompt_tokens\": 14,\n",
      "      \"total_tokens\": 31,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_560af6e559\",\n",
      "    \"id\": \"chatcmpl-CKgIR2ma9W6wi3YJlvetwG9wGIYsi\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The capital of South Korea is Seoul. North Korea's capital is Pyongyang.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"what is the captal of Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in OpenAICallbackHandler.on_llm_end callback: KeyError('total_tokens')\n",
      "Error in OpenAICallbackHandler.on_llm_end callback: KeyError('total_tokens')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: what is the recipt for soju\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Soju is a traditional Korean distilled spirit, typically made from grains or starches such as rice, barley, or sweet potatoes. While you can buy soju at stores, making it at home is a complex process that requires fermentation and distillation. Here’s a simplified overview of how soju is traditionally made:\\n\\n### Ingredients:\\n1. **Base Ingredient**: 2-3 cups of rice, barley, or sweet potatoes\\n2. **Water**: Enough to cook the base ingredient and for fermentation\\n3. **Nuruk**: A traditional Korean fermentation starter (can be found in Asian markets or online)\\n4. **Yeast**: Optional, depending on the fermentation process\\n\\n### Equipment:\\n- Large pot for cooking\\n- Fermentation container (glass or food-grade plastic)\\n- Distillation apparatus (if you want to distill it at home)\\n- Bottles for storage\\n\\n### Instructions:\\n\\n1. **Prepare the Base Ingredient**:\\n   - If using rice, rinse it thoroughly and soak it in water for several hours or overnight. Drain and steam the rice until fully cooked. If using sweet potatoes, peel, chop, and steam them until soft.\\n\\n2. **Mash the Base Ingredient**:\\n   - Once cooked, mash the rice or sweet potatoes to create a paste. Allow it to cool to room temperature.\\n\\n3. **Fermentation**:\\n   - Transfer the mashed base ingredient to a fermentation container. Add nuruk (follow the package instructions for the amount) and mix well. If using yeast, you can add it at this stage as well.\\n   - Add enough water to cover the mixture and stir to combine.\\n   - Cover the container with a cloth and let it ferment at room temperature for about 1-2 weeks. You should see bubbles forming, indicating fermentation.\\n\\n4. **Distillation** (if desired):\\n   - After fermentation, you can distill the mixture to create soju. This requires a distillation apparatus. Heat the fermented mixture slowly, and collect the vapor that condenses back into liquid. This process may need to be repeated to achieve the desired alcohol content.\\n\\n5. **Aging and Bottling**:\\n   - Once distilled, you can let the soju age for a while to develop its flavor. After aging, bottle the soju and store it in a cool, dark place.\\n\\n### Note:\\n- Home distillation may be illegal in some areas, so be sure to check your local laws before attempting to distill alcohol at home.\\n- The process can be quite complex and may require practice to perfect.\\n\\nIf you're looking for a simpler way to enjoy soju, consider purchasing it from a store, where you can find various flavors and brands!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Soju is a traditional Korean distilled spirit, typically made from grains or starches such as rice, barley, or sweet potatoes. While you can buy soju at stores, making it at home is a complex process that requires fermentation and distillation. Here’s a simplified overview of how soju is traditionally made:\\n\\n### Ingredients:\\n1. **Base Ingredient**: 2-3 cups of rice, barley, or sweet potatoes\\n2. **Water**: Enough to cook the base ingredient and for fermentation\\n3. **Nuruk**: A traditional Korean fermentation starter (can be found in Asian markets or online)\\n4. **Yeast**: Optional, depending on the fermentation process\\n\\n### Equipment:\\n- Large pot for cooking\\n- Fermentation container (glass or food-grade plastic)\\n- Distillation apparatus (if you want to distill it at home)\\n- Bottles for storage\\n\\n### Instructions:\\n\\n1. **Prepare the Base Ingredient**:\\n   - If using rice, rinse it thoroughly and soak it in water for several hours or overnight. Drain and steam the rice until fully cooked. If using sweet potatoes, peel, chop, and steam them until soft.\\n\\n2. **Mash the Base Ingredient**:\\n   - Once cooked, mash the rice or sweet potatoes to create a paste. Allow it to cool to room temperature.\\n\\n3. **Fermentation**:\\n   - Transfer the mashed base ingredient to a fermentation container. Add nuruk (follow the package instructions for the amount) and mix well. If using yeast, you can add it at this stage as well.\\n   - Add enough water to cover the mixture and stir to combine.\\n   - Cover the container with a cloth and let it ferment at room temperature for about 1-2 weeks. You should see bubbles forming, indicating fermentation.\\n\\n4. **Distillation** (if desired):\\n   - After fermentation, you can distill the mixture to create soju. This requires a distillation apparatus. Heat the fermented mixture slowly, and collect the vapor that condenses back into liquid. This process may need to be repeated to achieve the desired alcohol content.\\n\\n5. **Aging and Bottling**:\\n   - Once distilled, you can let the soju age for a while to develop its flavor. After aging, bottle the soju and store it in a cool, dark place.\\n\\n### Note:\\n- Home distillation may be illegal in some areas, so be sure to check your local laws before attempting to distill alcohol at home.\\n- The process can be quite complex and may require practice to perfect.\\n\\nIf you're looking for a simpler way to enjoy soju, consider purchasing it from a store, where you can find various flavors and brands!\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 549,\n",
      "                \"prompt_tokens\": 15,\n",
      "                \"total_tokens\": 564,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini\",\n",
      "              \"system_fingerprint\": \"fp_560af6e559\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--01eefb3e-c0cd-4772-bf53-db379a1b5c9b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"total_cost\": 0\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: what is the recipt for bread\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Here's a simple recipe for homemade bread:\\n\\n### Basic White Bread Recipe\\n\\n#### Ingredients:\\n- 4 cups all-purpose flour (plus extra for kneading)\\n- 2 1/4 teaspoons (1 packet) active dry yeast\\n- 1 1/2 teaspoons salt\\n- 1 tablespoon sugar\\n- 1 1/2 cups warm water (about 110°F or 43°C)\\n- 2 tablespoons olive oil or melted butter (optional)\\n\\n#### Instructions:\\n\\n1. **Activate the Yeast:**\\n   - In a small bowl, combine the warm water, sugar, and yeast. Stir gently and let it sit for about 5-10 minutes until it becomes frothy.\\n\\n2. **Mix the Dough:**\\n   - In a large mixing bowl, combine the flour and salt. Make a well in the center and add the yeast mixture and olive oil (if using).\\n   - Stir with a wooden spoon or spatula until the dough begins to come together.\\n\\n3. **Knead the Dough:**\\n   - Transfer the dough to a floured surface. Knead the dough for about 8-10 minutes until it is smooth and elastic. You may need to add a little more flour if the dough is too sticky.\\n\\n4. **First Rise:**\\n   - Place the kneaded dough in a lightly greased bowl, cover it with a clean kitchen towel or plastic wrap, and let it rise in a warm place for about 1-2 hours, or until it has doubled in size.\\n\\n5. **Shape the Loaf:**\\n   - Once the dough has risen, punch it down to release the air. Transfer it to a floured surface and shape it into a loaf. Place it in a greased 9x5-inch loaf pan.\\n\\n6. **Second Rise:**\\n   - Cover the loaf with a towel and let it rise again for about 30-45 minutes, or until it has risen just above the rim of the pan.\\n\\n7. **Preheat the Oven:**\\n   - Preheat your oven to 375°F (190°C).\\n\\n8. **Bake the Bread:**\\n   - Once the dough has risen, bake it in the preheated oven for about 30-35 minutes, or until the bread is golden brown and sounds hollow when tapped on the bottom.\\n\\n9. **Cool:**\\n   - Remove the bread from the oven and let it cool in the pan for about 10 minutes. Then transfer it to a wire rack to cool completely before slicing.\\n\\n### Enjoy!\\nYou can enjoy this bread plain, with butter, or use it for sandwiches. Feel free to experiment by adding herbs, seeds, or other ingredients to customize your bread!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Here's a simple recipe for homemade bread:\\n\\n### Basic White Bread Recipe\\n\\n#### Ingredients:\\n- 4 cups all-purpose flour (plus extra for kneading)\\n- 2 1/4 teaspoons (1 packet) active dry yeast\\n- 1 1/2 teaspoons salt\\n- 1 tablespoon sugar\\n- 1 1/2 cups warm water (about 110°F or 43°C)\\n- 2 tablespoons olive oil or melted butter (optional)\\n\\n#### Instructions:\\n\\n1. **Activate the Yeast:**\\n   - In a small bowl, combine the warm water, sugar, and yeast. Stir gently and let it sit for about 5-10 minutes until it becomes frothy.\\n\\n2. **Mix the Dough:**\\n   - In a large mixing bowl, combine the flour and salt. Make a well in the center and add the yeast mixture and olive oil (if using).\\n   - Stir with a wooden spoon or spatula until the dough begins to come together.\\n\\n3. **Knead the Dough:**\\n   - Transfer the dough to a floured surface. Knead the dough for about 8-10 minutes until it is smooth and elastic. You may need to add a little more flour if the dough is too sticky.\\n\\n4. **First Rise:**\\n   - Place the kneaded dough in a lightly greased bowl, cover it with a clean kitchen towel or plastic wrap, and let it rise in a warm place for about 1-2 hours, or until it has doubled in size.\\n\\n5. **Shape the Loaf:**\\n   - Once the dough has risen, punch it down to release the air. Transfer it to a floured surface and shape it into a loaf. Place it in a greased 9x5-inch loaf pan.\\n\\n6. **Second Rise:**\\n   - Cover the loaf with a towel and let it rise again for about 30-45 minutes, or until it has risen just above the rim of the pan.\\n\\n7. **Preheat the Oven:**\\n   - Preheat your oven to 375°F (190°C).\\n\\n8. **Bake the Bread:**\\n   - Once the dough has risen, bake it in the preheated oven for about 30-35 minutes, or until the bread is golden brown and sounds hollow when tapped on the bottom.\\n\\n9. **Cool:**\\n   - Remove the bread from the oven and let it cool in the pan for about 10 minutes. Then transfer it to a wire rack to cool completely before slicing.\\n\\n### Enjoy!\\nYou can enjoy this bread plain, with butter, or use it for sandwiches. Feel free to experiment by adding herbs, seeds, or other ingredients to customize your bread!\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 553,\n",
      "                \"prompt_tokens\": 14,\n",
      "                \"total_tokens\": 567,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini\",\n",
      "              \"system_fingerprint\": \"fp_560af6e559\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--2f72bd17-d008-43b3-b8a2-a89dc8dd7d41-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"total_cost\": 0\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 0\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n",
      "0.0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#serialization\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "chat : Any = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a: Any = chat.predict(\"what is the recipt for soju\")\n",
    "    b: Any = chat.predict(\"what is the recipt for bread\")\n",
    "\n",
    "print(usage)\n",
    "print(usage.total_cost)        # 전체 비용\n",
    "print(usage.total_tokens)      # 토큰 총합\n",
    "print(usage.prompt_tokens)     # 프롬프트에 사용된 토큰수\n",
    "print(usage.completion_tokens) # 모델이 쓴 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='howarre you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#memory\n",
    "#ConversationBufferMemory\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\" : \"Hi\"}, {\"outpu\" : \"howarre you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1720\\3339732550.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "#ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "add_message(\"1\",\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    add_message(str(1), str(1))\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1720\\134659016.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: Hi I'm KwanTae I live in South Korea\\nAI: Wow that is so cool\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: Hi I'm KwanTae I live in South Korea\\nAI: Wow that is so cool\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LLMChain > llm:ChatOpenAI] [625ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces themselves as KwanTae from South Korea. The AI responds by expressing admiration for their location.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces themselves as KwanTae from South Korea. The AI responds by expressing admiration for their location.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 23,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 180,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--f8f14e41-b006-482a-8443-f396390747f0-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 23,\n",
      "      \"prompt_tokens\": 157,\n",
      "      \"total_tokens\": 180,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LLMChain] [627ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces themselves as KwanTae from South Korea. The AI responds by expressing admiration for their location.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#ConversationSummaryMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm KwanTae I live in South Korea\", \"Wow that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as KwanTae from South Korea. The AI responds by expressing admiration for their location.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m88 packages\u001b[0m \u001b[2min 432ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m networkx \u001b[2m(1.9MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m networkx\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 602ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 377ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "async def add_message(input, output):\n",
    "    await memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "async def main():\n",
    "    await add_message(\"Hi I'm KwanTae I live in South Korea\", \"Wow that is so cool\")\n",
    "    history = await memory.load_memory_variables({})\n",
    "    print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"\",\n",
      "  \"input\": \"who is KwanTae\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\n\\nLast line of conversation (for extraction):\\nHuman: who is KwanTae\\n\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LLMChain > llm:ChatOpenAI] [1.01s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"KwanTae\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"KwanTae\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 4,\n",
      "                \"prompt_tokens\": 405,\n",
      "                \"total_tokens\": 409,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-CKgzH5NgPzxvs8PXEcEIKaDT9mZfA\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--a9dd4038-f7e3-4445-a01c-1cbcb2fea823-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 405,\n",
      "              \"output_tokens\": 4,\n",
      "              \"total_tokens\": 409,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 4,\n",
      "      \"prompt_tokens\": 405,\n",
      "      \"total_tokens\": 409,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-CKgzH5NgPzxvs8PXEcEIKaDT9mZfA\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LLMChain] [1.01s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"KwanTae\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\" : \"who is KwanTae\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Memory on LLM Chain\n",
    "\n",
    "#LLM = off the shlf : General Perpose chain (일반적인 목적을 가지는 체인)\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "you are a helpful AI taling a haman.\n",
    "\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "Your:\n",
    "\"\"\"\n",
    "\n",
    "chain: Any = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # prompt=PromptTemplate.from_template(\"{question}\")\n",
    "    prompt=PromptTemplate.from_template(template=template)\n",
    ")\n",
    "\n",
    "chain.predict(question=\"I live in seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI: You live in Seoul, which is the capital city of South Korea. It's known for its bustling neighborhoods, delicious food, and historical sites. Do you have a favorite area or landmark in the city?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"where's do I live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: The human mentions living in Seoul, and the AI responds positively, highlighting Seoul's vibrant culture and history, and asks what the human enjoys most about living there.\\nHuman: where's do I live\\nAI: AI: You live in Seoul, which is the capital city of South Korea. It's known for its bustling neighborhoods, delicious food, and historical sites. Do you have a favorite area or landmark in the city?\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChatBasedMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt: Any = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain: Any = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\n",
      "Human: where's i live\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"If you're looking for specific information about your location in Seoul, I can't access personal data or know your exact address. However, I can help you with general information about neighborhoods, attractions, or anything else related to Seoul. Just let me know what you need!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"where's i live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LCEL Based Memory(LangChain Expression Language)\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "    print(result)\n",
    "\n",
    "#chain.invoke(\n",
    "#    {\n",
    "#        \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"],\n",
    "#        \"question\" : \"My name is ksm\"\n",
    "#    }\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, ksm! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 25, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CKhn5W77jwHCQercTkVTcjn6155gT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--73638309-31ff-490b-b8e8-ab2629a671a0-0' usage_metadata={'input_tokens': 25, 'output_tokens': 15, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is ksm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='My name is ksm', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice to meet you, ksm! How can I assist you today?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
